# Replication

	Replication is the ultimate standard but hard to do.
	Reproducibility is validation of the data analysis:
	1) Analytic data is available
	2) Analytic code is available
	3) Documentation of code and data
	4) Standard means of distribution
	In one sententence: write everything down.
	
	# Choose an appropriate dataset
	Descriptive - a whole population
	Exploratory - a random sample with many variables measured
	Inferential - the right population, randomly sampled
	Predictive - a training and test data set from the same population
	Causal - data from a randomized study
	Mechanistic - data about all components of the system
	
	Quote the reference and the date it was accessed

	
# Example of spam classification

	Lead with the question
		- Can I use quantitative characteristics of the emails to classify them as SPAM/HAM?
	Describe the approach
		Collected data from UCI -> created training/test sets
		Explored relationships
		Choose logistic model on training set by cross validation
		Applied to test, 78% test set accuracy
	Interpret results
		Number of dollar signs seems reasonable, e.g. "Make money with Viagra $ $ $ $!"
	Challenge results
		78% isn't that great
		I could use more variables
		Why logistic regression?

	## Code	
	# Create train and test set
	library(kernlab); data("spam"); set.seed(3435);
	trainIndicator = rbinom(4601, size = 1, prob = 0.5)
	trainSpam = spam[trainIndicator == 1, ]
	testSpam = spam[trainIndicator == 0, ]

	# Which predictor has minimum cross-validated error?
	trainSpam$numType = as.numeric(trainSpam$type) - 1
	costFunction = function(x, y) sum(x != (y > 0.5))
	cvError = rep(NA, 55)
	library(boot)
	for (i in 1:55) {
		lmFormula = reformulate(names(trainSpam)[i], response = "numType")
		glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
		cvError[i] = cv.glm(trainSpam, glmFit, costFunction, 2)$delta[2]
	}
	names(trainSpam)[which.min(cvError)]
	
	# Use the best model from the group (charDollar)
	predictionModel = glm(numType ~ charDollar, family = "binomial", data = trainSpam)
	
	# Get predictions on the test set
	predictionTest = predict(predictionModel, testSpam)
	predictedSpam = rep("nonspam", dim(testSpam)[1])
	
	# Classify as `spam' for those with prob > 0.5
	predictedSpam[predictionModel$fitted > 0.5] = "spam"
	
	## Classification table
	table(predictedSpam, testSpam$type)
	## Error rate
	(TN + FP)/(TP + TN + FP + FN)
	
	
# Markdown
	
	*italics*, **bold**
	## secondary header		### tertiary header
	- item1		- item2		- item3
	1. item1	2. item2	3. item3
	[link](URL)
	Advanced URl like [this][1]!
	[1]: http://wwww.google.com "google"
	newlines are double spaces		
	
	R code options
		```{r nameofthechunk, echo=FALSE, results="hide"}
		R code
		```
	For plots, you have options like fig.width, fig.height
	For tables, use xtable
		```{r showtable, results="asis"}
		library(xtable)
		xt <- xtable(summary(fit))
		print(xt, type = "html")
		```
	Global options
		```{r setoptions, echo=FALSE}
		opts_chunk$set(echo = FALSE, results = "hide")
		```
	Caching
		cache=TRUE
		Note: dependencies are not checked explicitly!!
	You can use inline text computations
		`r time`, `r rand`

	# knitr
	Literate programming for R and documentation (Rmarkdown, LateX, HTML).
	Can export to PDF, HTML.
	
	.Rmd -> .md -> .html
	In Rstudio:
		R studio -> new R markdown -> saves as .Rmd -> knit to HTML
	R code:
		library(knitr); setwd();
		knit2html("doc.Rmd")
		browseURL("doc.html")
		

 # Organising results
	
	People are busy, breakdown the results of an analysis into different levels of detail:
		Abstract
		Body / Results
		Supplementary Materials
		Code / Data
		
	# Rpubs
	To publish .html code from R, you can use Rpubs. 
	After knitr -> html, RStudio has a publish icon that will link to Rpubs.
	
	# Check list
	- Don't do things by hands: document it, it must be reproducible and/or automated.
	- Use as few interractive software as possible.
	+ Teach a computer (so that it is reproducible)
	+ Use version control in small chunks
	+ Detail your software environmment (from OS to external dependencies)
		in R: sessionInfo()
	+ Set the seed
	+ Think about the entire pipeline. Think about how to get to the end.
		raw data -> processed data -> analysis -> report
	
	# Summary: Checklist
	* Are we doing good science?
	* Was any part of this analysis done by hand?
	  - If so, are those parts *precisely* document?
	  - Does the documentation match reality?
	* Have we taught a computer to do as much as possible (i.e. coded)?
	* Are we using a version control system?
	* Have we documented our software environment?
	* Have we saved any output that we cannot reconstruct from original
	  data + code?
	* How far back in the analysis pipeline can we go before our results
	  are no longer (automatically) reproducible?


# Caching

	#cacher package
	Evaluates code and store intermediate results
	Uses SHA-1 hash to track changes
	Allows others to clone an analysis
	Package file is zipped and can be distributed
	Allows transparent inspection of the analysis
	Efficiently loads only the necessary data objects
	
	# Example
		library(cacher)
		clonecache(id = "092dcc7dda4b93e42f23e038a60e1d44dbec7b3f”)
		clonecache(id = “092d”) # Same as above
		
		# The cache was cloned, now load it
		showfiles()
		sourcefile("top20.R")
		
		# Examining code (forward approach)
		code()
		graphcode()
		
		# Get the code that made the data (backwards approach)
		objectcode(“data”)

	# runcode()
	Executes code in the source file
	Expression resulting in objects are not run but lazy-loaded
	Expression not resulting in objects are evaluated
	
	# checkcode()
	Evaluates all expressions from scratch (no lazy-loading)
		Setting seeds is critical
	
	# Example
	loadcache()
	ls()
	cities	# will transfer from cache
	
	
# Case Study

	The most common mistakes are simple.
		Confounding in the Experimental Design
		Mixing up the sample labels
		Mixing up the gene labels
		Mixing up the group labels
		(Most mixups involve simple switches or offsets)
	This simplicity is often hidden.
	Incomplete documentation


	  
		
		

	