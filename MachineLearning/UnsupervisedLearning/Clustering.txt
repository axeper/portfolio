## Clustering

### Decision making
- Should the observations or features first be standardized in some way?
- In the case of K-means clustering, how many clusters should we look for in the data?
  - Are the clusters we obtained noise or are true subgroups?
- In the case of hierarchical clustering,
  – What dissimilarity measure should be used?
  – What type of linkage should be used?
  – Where should we cut the dendrogramin order to obtain clusters?
  
  
### Similarity measures

Choose a metric
- Euclidian distance.
- Manhattan distance.
- Cosine distance (useful for text classification, ignore the difference in scale).
- Jaccard distance = 1 - |X & Y| / |X | Y|. Useful for sets similarity.
- Edit distance: how many insert/delete/replace between two strings.

Be mindful of the curse of dimensionality when looking for close neighbors.




## K-means

- Set K clusters randomly
- Choose a distance metric (e.g. euclidian distance) and compute the distance between each clusters/centroids
- Assign data to the closest centroid
- Recompute the clusters positions

Or:
Partition the observations into K cluster such that the total within-cluster variation (for instance the squared Euclidian distance) is minimized.

Note: The random centroid initial location finds a local and not a global optimum because of the initial (random) cluster assignment. Then the best solution is selected after multiple runs.

K can be chosen with the silhouette, an elbow curve and the sum of squared distance, ... 
Example: Within Sum of Squares (WSS) metric
	WSS = sum( distance(centroid, points)^2)
	wss <- numeric(15)
	for (k in 1:15) wss[k] <- sum(kmeans(kmdata, centers=k, nstart=25)$withinss)

```{r}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj); kmeansObj$cluster;

par(mar=rep(0.2,4))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)

# one liner
plot(x,y,col=kmeans(dataFrame,6)$cluster,pch=19,cex=2)
```

Caution:
- What object attributes should be included or excluded in the analysis?
- What unit of measure (for example, miles or kilometers) should be used for each attribute? Height in m or cm can be clustered differently!
- Do the attributes need to be rescaled so that one attribute does not have a disproportionate effect on the results?

k-means fails when the clusters are not "circular", i.e. when the assumption that all directions are equally important for each cluster is not met. 
Complex shapes (like two moons) usually do not work either. Note: you can use more clusters to find an expressive representation for complex shapes.

	
### Spherical k-means: k-means + cosine distance

```{R}
install.packages("skmeans",dependencies = TRUE)
library(skmeans)
winedata <- read.csv("WineKMC.csv")

# Preprocess
winedata[is.na(winedata)] <- 0
winedata.transposed <- t(winedata[,8:107])

# Compute spherical kmeans
winedata.clusters <- skmeans(winedata.transposed, 5, method="genetic")

# Group by clusters
winedata.clustercounts <-
t(aggregate(winedata.transposed,
by=list(winedata.clusters$cluster),sum)[,2:33])
winedata.desc.plus.counts <- 
cbind(winedata[,1:7],winedata.clustercounts)

# Show data by the first cluster
winedata.desc.plus.counts[order(-winedata.desc.plus.counts[,8]),]
```


### K-medians

Minimize absolute deviations: k-medians + manhattan distance(binary) or cosine distance(asymmetric, i.e. more weight on 1 than 0)

k-means does not handle categorical data. In such cases, k-modes [3] is a commonly used method for clustering categorical data based on the number of differences in the respective components of the attributes. For example, if each object has four attributes, the distance from (a, b, e, d) to (d, d, d, d) is 3. In R, the function kmode() is implemented in the klaR package

A third partitioning method is known as Partitioning around Medoids (PAM) [4]. In general, a medoid is a representative object in a set of objects. In clustering, the medoids are the objects in each cluster that minimize the sum of the distances from the medoid to the other objects in the cluster. The advantage of using PAM is that the “center” of each cluster is an actual object in the dataset. PAM is implemented in R by the pam() function included in the cluster R package. The fpc R package includes a function pamk(), which uses the pam() function to find the optimal value for k.
	k-medoids is the problem specification. It may be a np-hard problem.
	PAM is one algorithm to find a local minimum for the k-medoids problem. Maybe not the optimum, but faster than exhaustive search.

	The fpc R package includes a function, dbscan(), to perform density-based clustering analysis. Density-based clustering can be useful to identify irregularly shaped clusters.




## Hierarchical Clustering / Agglomerative Clustering

An issue with K-means is that we have to choose K. On the other hand, hierarchical clustering is a bottom-up, agglomerative approach to clustering. The lower in the dendogram, the closer the observations. Proximity is defined by the vertical axis, no the horizontal one.

Unfortunately, this family of methods still fails at separating complex shapes.


### Algorithm
- Find closest two things -> merge them -> find next closest
- Requires : distance metric, linkage (merging criterion)
- Produces: a dendogram, a tree showing how close things are and that can be trimmed at different levels

Distance metric (pick one that makes sense):
	Continuous: euclidean distance (sqrt(sum(distance)^2))
	Continuous: correlation similarity
	Binary: manhattan distance (sum of absolute distance)
Scaling: 
	Do we scale the variables? 
	With what values? Frequency, price, ...?
Linkage:
	Average: mean intercluster dissimiliarity.
	Complete: maximal intercluster dissimiliarity.
	Single: minimal intercluster dissimiliarity.
	Centroid: dissimilarity between the centroid for cluster A and the centroid for cluster B.
	(scikit-learn) ward: clusters merge such that the variance within all clusters increases the least. 

	
```{r}
set.seed(1234); par(mar=c(0,0,0,0))
x <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))

dataFrame <- data.frame(x=x,y=y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
plot(as.dendrogram(hClustering))

# Prettier
myplclust(hClustering,lab=rep(1:3,each=4),lab.col=rep(1:3,each=4))
# Even prettier
http://gallery.r-enthusiasts.com/RGraphGallery.php?graph=79

# Heatmap
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix, cm.colors(25))
```




## DBSCAN (Density-based spatial clustering of applications with noise)
Identify points in dense regions of the feature space. Clusters form dense regions of data, separated by regions that are relatively empty.
There are three kinds of points: core points, points that are within distance eps of core points (called boundary points), and noise.

Parameters:
- eps: increase means that more points will be included in a cluster. This makes clusters grow, but might also lead to multiple clusters joining into one. Setting eps to be very large will result in all points forming a single cluster.
Finding a good setting for eps is sometimes easier after scaling the data using StandardScaler or MinMaxScaler, as using these scaling techniques will ensure that all features have similar ranges.
- min_samples: increase means that fewer points will be core points, and more points will be labeled as noise. It determines whether points in less dense regions will be labeled as outliers or as their own clusters.




## Comparing clustering
There are metrics that can be used to assess the outcome of a clustering algorithm relative to a ground truth clustering, the most important ones being the adjusted rand index (ARI) and normalized mutual information (NMI), which both provide a quantitative measure between 0 and 1.

When no ground truth exist. We can use the silhouette (doesn't work perfectly), robustness-based clustering metrics (not implemented in scikit-learn)

