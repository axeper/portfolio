## Ensemble model

We assume that many uncorrelated predictions might provide complementary information when taken together than any individual expert.
More complexity gives higher higher variance but lower bias (overfit), lower complexity gives lower variance but higher bias. 
Preventing overfitting: implement complexity control.
If the models are correlated, ensembled models will not perform well.


### Bagging

- Construct B regression trees using B bootstrapped (taking repeated samples from the training set with replacement) and average the predictions or take a majority vote. B = 100 is good enough
- Out-of-Bag Error Estimation: take 1/3 of the data as out-of-bag observations, fit on the other 2/3. If B is large enough, this estimation is close to the CV.
- Bagging lose the interpretability of decision trees. Still we can measure the variable importance (mean decrease in Gini index or RSS) to compare predictors relatively to each others.


### Boosting

- Boosting a decision tree is a slow fit where the trees are grown sequentially.
- Fit a tree on the data, then fit a new model on the residuals of the past tree. Do that B times. Update residuals and trees according to a shrinkage variable lambda.
  - The number of trees B. Boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B.
  - The shrinkage parameter λ, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001. Very small λ can require using a very large value of B in order to achieve good performance.
  - The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump, consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth, and controls the interaction order of the boosted model, since d splits can involve at most d variables.

- Use the whole dataset and on each iteration change the weights.
- N data with initial weight 1/N
- Find the feature with the lowest weighed error.
- Adjust each weight: decrease the weight when correctly predicted, increase the weight when incorrectly predicted

[Adaboost on Wikipedia](http://en.wikipedia.org/wiki/AdaBoost)

# Example
http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf


### Bagging vs Boosting
Boosting needs less trees, slightly more risk of overfitting, cannot be parallelized. 




## Combining predictors

You can combine classifiers by averaging/voting
Combining classifiers improves accuracy, reduces interpretability

# Approaches for combining classifiers
1. Combine similar classifiers 
  - Bagging, boosting, random forests
2. Combining different classifiers
  - Model stacking
  - Model ensembling

```{r}
mod1 <- train(wage ~.,method="glm",data=training)
mod2 <- train(wage ~.,method="rf",data=training, trControl = trainControl(method="cv"),number=3)
pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
VS
predDF <- data.frame(pred1,pred2,wage=testing$wage)
combModFit <- train(wage ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)

# Error	
sqrt(sum((pred1-testing$wage)^2))
sqrt(sum((pred2-testing$wage)^2))
sqrt(sum((combPred-testing$wage)^2))

# Predict on validation data set
pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
combPredV <- predict(combModFit,predVDF)

sqrt(sum((pred1V-validation$wage)^2))
sqrt(sum((pred2V-validation$wage)^2))
sqrt(sum((combPredV-validation$wage)^2))
```




