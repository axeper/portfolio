## Dimension Reduction

Project the p predictors into a M-dimensional subspace (with M < p). These projections are used to fit a new linear models.
All dimension reduction methods work in two steps. First, the transformed predictors Z1, Z2, . . . , ZM (linear combinations of our original p predictors) are obtained. Second, the model is fit using these M predictors.


### High dimension data
- It is possible to perfectly fit the training data in the high-dimensional setting, the resulting linear model will perform extremely poorly on an independent test set, and therefore does not constitute a useful model. 
- When p > n or p â‰ˆ n, a simple least squares regression line is too flexible and hence overfits the data. R^2 will increase, train MSE will decrease but there is no guarantee that test MSE will decrease.
- Less flexible methods such as forward stepwise selection, ridge regression, the lasso, and principal components regression, are particularly useful for performing regression in the high-dimensional setting.
(1) regularization or shrinkage plays a key role in high-dimensional problems
(2) appropriate tuning parameter selection is crucial for good predictive performance
(3) curse of dimensionality: the test error tends to increase as the dimensionality of the problem (i.e. the number of features or predictors) increases, unless the additional features are truly associated with the response.
  - In general, adding additional signal features that are truly associated with the response will improve the fitted model, in the sense of leading to a reduction in test set error. However, adding noise features that are not truly associated with the response will lead to a deterioration in the fitted model, and consequently an increased test set error.



### Principal Components Analysis
The first principal component direction of the data is that along which the observations vary the most. That is, if we projected the observations onto this line, then the resulting projected observations would have the largest possible variance; equivalently projecting the observations onto any other line would yield projected observations with lower variance.
- Z1 = a1 * (x1 - mean(x1)) + a2 * (x2 - mean(x2))


### Principal Components Regression
- In PCR instead of regressing the dependent variable on the independent variables directly, the principal components of the independent variables are used.
- The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the directions in which X1, . . .,Xp show the most variation are the directions that are associated with Y.
- Shrinkage vs PCR: PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response. On the other hand, when a lot of components are needed, one can expect shrinkage to perform better than PCR.
- We note that even though PCR provides a simple way to perform regression using M < p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features.
- In PCR, the number of principal components, M, is typically chosen by CV.
- When performing PCR, we generally recommend standardizing each predictor prior to generating the principal components


### Partial Least Squares
Unlike PCR, PLS identifies these new features in a supervised way. The latent structure must be present in X (the predictor) and Y (the response).
