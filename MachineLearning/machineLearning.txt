# Resources

	* [List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)
	* [List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)
	* [Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)
	* [Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)
	* [Kaggle - machine learning competitions](http://www.kaggle.com/)


# Steps

	question -> 
	input data -> 
		garbage in, garbage out
		more data > better models)
	features ->
		Choose sparse (leads to compression) but relevant features
		Choice is based on application knowledge
		Don't rely on automated but uncomprehensible features 
		Study outliers
		Don't throw away information
	algorithm -> 
		Prefer a generic algorithm to a specific one
	parameters -> 
	evaluation
	

# In sample versus out of sample

	In Sample Error: The error rate you get on the same data set you used to build your predictor. Sometimes called resubstitution error.
	Out of Sample Error: The error rate you get on a new data set. Sometimes called generalization error.
	
	Out of sample error is key because it prevents overfitting on noise.
	Prefer simple rules to complicated ones.
	
	Use train and test for cross-validation, but validation only for results.
	Split sets as 60%, 20%, 20%. Split them randomly.
	
	# Cross-validation
	Split the training set into train/test, build on train, eval on test, repeat and average. Don't touch the test set for cross-validation.
	Use chunks when working with time series.
	K-fold: large K is high variance, low bias but low K is less variance, more bias
	Another strategy is "Leave one out"
	
	
# Types of error

	Sensitivity	- TP / (TP + FN)
	Specificty	- TN / (FP + TN)
	Positive Predictive Value - TP / (TP + FP)
	Negative Predictive Value - TN / (FN + TN)
	Accuracy	- (TP + TN) / (TP + FP + FN + TN)
	
	Mean squared error (MSE):
	1/n * sum(Prediction_i - Truth_i)^2
	Root mean squared error (RMSE):
	sqrt( 1/n * sum(Prediction_i - Truth_i)^2)
	
	## Common error measures
	1. Mean squared error (or root mean squared error)
	  * Continuous data, sensitive to outliers
	2. Median absolute deviation 
	  * Continuous data, often more robust
	3. Sensitivity (recall)
	  * If you want few missed positives
	4. Specificity
	  * If you want few negatives called positives
	5. Accuracy
	  * Weights false positives/negatives equally
	6. Concordance
	  * One example is [kappa](http://en.wikipedia.org/wiki/Cohen%27s_kappa)
	
	# ROC curve
	Senstivity vs (1 - Specificity).
	The bigger the area, the better the curve: 0.5 is random, 1 is perfect, 0.8 is good
	
	
# caret package

	# Example
	library(caret); library(kernlab); data(spam)
	inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
	training <- spam[inTrain,]; testing <- spam[-inTrain,]
	modelFit <- train(type ~.,data=training, method="glm"); modelFit$finalModel
	predictions <- predict(modelFit,newdata=testing)
	confusionMatrix(predictions,testing$type)

	# K-fold
	folds <- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=TRUE)
	folds <- createFolds(y=spam$type,k=10,list=TRUE,returnTrain=FALSE)

	# Resampling 
	folds <- createResample(y=spam$type,times=10,list=TRUE)
	
	# Time Slices
	tme <- 1:1000; folds <- createTimeSlices(y=tme,initialWindow=20,horizon=10)
	
	# Training options
	args(train.default)
	function (x, y, method = "rf", preProcess = NULL, ..., weights = NULL, 
		metric = ifelse(is.factor(y), "Accuracy", "RMSE"), maximize = ifelse(metric == "RMSE", FALSE, TRUE), trControl = trainControl(), tuneGrid = NULL, tuneLength = 3) 

	# metric
	RMSE, Rsquared for continuous; accuracy, kappa for categorical
	
	# trainControl
	args(trainControl)
		method
			boot = bootstrapping
			boot632 = bootstrapping with adjustment
			cv = cross validation
			repeatedcv = repeated cross validation
			LOOCV = leave one out cross validation
		number
			For boot/cross validation
			Number of subsamples to take
		repeats
			Number of times to repeate subsampling
			If big this can slow things down
		
	# Feature plot
	featurePlot(x=training[,c("age","education","jobclass")],y = training$wage,plot="pairs")
	
	# Qplot
	qq <- qplot(age,wage,colour=education,data=training)
	qq +  geom_smooth(method='lm',formula=y~x)

	# cut2, making factors (Hmisc package) - split into g quantiles
	library("Hmisc"); cutWage <- cut2(training$wage,g=3)
	t1 <- table(cutWage,training$jobclass)
	prop.table(t1,1)

	# Boxplots with points overlayed
	p1 <- qplot(cutWage,age, data=training,fill=cutWage,geom=c("boxplot"))
	p2 <- qplot(cutWage,age, data=training,fill=cutWage,geom=c("boxplot","jitter"))
	grid.arrange(p1,p2,ncol=2)

	
	* Caret tutorials:
	* http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf
	* http://cran.r-project.org/web/packages/caret/vignettes/caret.pdf
	* http://www.jstatsoft.org/v28/i05/paper
	* [Model training and tuning](http://caret.r-forge.r-project.org/training.html)


# Preprocessing

	Training and test must be processed in the same way	
	Test transformations will likely be imperfect
	Especially if the test/training sets collected at different times
	Preprocessing factors are very tricky.

	# Standardizing
	trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
	testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
	Or
	preObj <- preProcess(training[,-58],method=c("center","scale"))
	trainCapAveS <- predict(preObj,training[,-58])$capitalAve
	testCapAveS <- predict(preObj,testing[,-58])$capitalAve
	Or
	mFit <- train(type ~.,data=training,preProcess=c("center","scale"),method="glm")

	# Standardizing - Box-Cox transforms (transforms highly skewed data into normal)
	preObj <- preProcess(training[,-58],method=c("BoxCox"))
	trainCapAveS <- predict(preObj,training[,-58])$capitalAve
	par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)

	# Imputing missing data with knn
	preObj <- preProcess(training[,-58],method="knnImpute")
	capAve <- predict(preObj,training[,-58])$capAve

	[preprocessing with caret](http://caret.r-forge.r-project.org/preprocess.html)
	

# Covariate (extraction of features)
	
	The balancing act is summarization vs. information loss.
		* Images: Edges, corners, blobs, ridges
		* People: Height, weight, hair color, sex, country of origin
	The more knowledge of the system you have the better the job you will do. 
	When in doubt, create more features than less
	Can be automated, but use caution because interpretation is hard.
	
	# Adding covariates (features)
	A good approach is through exploratory analysis (plotting/tables)
	Convert factors into dummy variables
		dummies <- dummyVars(wage ~ jobclass,data=training)
		head(predict(dummies,newdata=training))
	Remove near zero covariates (i.e. features that do not vary)
		nsv <- nearZeroVar(training,saveMetrics=TRUE)
	
	Use splines to fit the data:
		library(splines)
		bsBasis <- bs(training$age,df=3)
		# Fit splines on the plot
		lm1 <- lm(wage ~ bsBasis,data=training)
		plot(training$age,training$wage,pch=19,cex=0.5)
		points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
		# Apply on the test
		predict(bsBasis,age=testing$age)

		
# PCA

	# Example
	library(caret); library(kernlab); data(spam)
	inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
	training <- spam[inTrain,]; testing <- spam[-inTrain,]

	# Find which value are well correlated with each others
	# Remove anything on the diagonal (cor(x,x) = 1)
	M <- abs(cor(training[,-58]))
	diag(M) <- 0
	which(M > 0.8,arr.ind=T)
	
	# We find that these two are highly correlated
	plot(spam[,34],spam[,32])
	
	Let's use a weighted combination to reduce noise and the number of predictors.
	Similarly:
		1) Find a new set of multivariate variables that are uncorrelated and explain as much variance as possible. (statistical goal)
		2) Putting all the variables together in one matrix, we want to find the best matrix created with fewer variables that explains the original data. (data compression goal)	
	Solutions:
		SVD: If X is a matrix with each variable in a column and each observation in a row then the SVD is a "matrix decomposition"
		X = U*D*transpose(V)
		where the columns of U are orthogonal (left singular vectors), the columns of V are orthogonal (right singluar vectors) and D is a diagonal matrix (singular values). 
		
		PCA: The principal components are equal to the right singular values if you first scale (subtract the mean, divide by the standard deviation) the variables.
		in R:
			smallSpam <- spam[,c(34,32)]
			prComp <- prcomp(smallSpam)
			plot(prComp$x[,1],prComp$x[,2])
			prComp$rotation

		# PCA on SPAM data
			typeColor <- ((spam$type=="spam")*1 + 1)
			prComp <- prcomp(log10(spam[,-58]+1))
			plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
			Or with caret:
			preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
			spamPC <- predict(preProc,log10(spam[,-58]+1))
			plot(spamPC[,1],spamPC[,2],col=typeColor)
			# Train, test
			trainPC <- predict(preProc,log10(training[,-58]+1))
			modelFit <- train(training$type ~ .,method="glm",data=trainPC)
			testPC <- predict(preProc,log10(testing[,-58]+1))
			confusionMatrix(testing$type,predict(modelFit,testPC))
			One-liner:
			modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
			confusionMatrix(testing$type,predict(modelFit,testing))
		
	Conlcusions:
	* Most useful for linear-type models
	* Can make it harder to interpret predictors
	* Watch out for outliers! 
	  * Transform first (with logs/Box Cox)
	  * Plot predictors to identify problems


# Simple regression
	
	library(caret); data(faithful); set.seed(333)
	inTrain <- createDataPartition(y=faithful$waiting,p=0.5, list=FALSE)
	trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]
	lm1 <- lm(eruptions ~ waiting,data=trainFaith)	
	
	# Plot the regression
	plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue", 		 xlab="Waiting",ylab="Duration")
	lines(trainFaith$waiting,lm1$fitted,lwd=3)
	
	# Predict
	coef(lm1)[1] + coef(lm1)[2]*80
	Or:
	newdata <- data.frame(waiting=80)
	predict(lm1,newdata)
	
	# Calculate RMSE on training and testing
	sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
	sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))

	# Prediction intervals
	pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
	ord <- order(testFaith$waiting)
	plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
	matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty = c(1,1,1), lwd=3)

	Or:
	modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
	summary(modFit$finalModel)

	
# Regression with multivariate
	
	library(ISLR); library(ggplot2); library(caret);
	data(Wage); Wage <- subset(Wage,select=-c(logwage))
	inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
	training <- Wage[inTrain,]; testing <- Wage[-inTrain,]

	featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,plot="pairs")
	# Explore 2D plots with colors
	qplot(age,wage,colour=jobclass,data=training)
	qplot(age,wage,colour=education,data=training)

	# Multivariate fitting
	modFit <- train(wage ~ age + jobclass + education,method = "lm",data=training)
	finMod <- modFit$finalModel

	# Explore residuals
	plot(finMod,1,pch=19,cex=0.5,col="#00000010")
	qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
	plot(finMod$residuals,pch=19)

	# Prediction on test set
	pred <- predict(modFit, testing)
	qplot(wage,pred,colour=year,data=testing)

	
	# Using ALL the covariates
	modFitAll<- train(wage ~ .,data=training,method="lm")
	pred <- predict(modFitAll, testing)
	qplot(wage,pred,data=testing)


# Predicting with trees

	# Key ideas
	Iteratively split variables into groups
	Evaluate "homogeneity" within each group
	Split again if necessary
	+ Easy to interpret
	+ Better performance in nonlinear settings
	- Without pruning/cross-validation can lead to overfitting
	- Harder to estimate uncertainty
	- Results may be variable
	
	# Basic algorithm
	1. Start with all variables in one group
	2. Find the variable/split that best separates the outcomes
	3. Divide the data into two groups ("leaves") on that split ("node")
	4. Within each split, find the best variable/split that separates the outcomes
	5. Continue until the groups are too small or sufficiently "pure"

	# Measures of impurity
	hat(p_mk) = 1/N_m * sum{x_i in Leaf_m} \mathbb{1}(y_i = k)
		In the leaf m, you have N_m objects. 
		\mathbb{1}(y_i = k) is the number of class k in leaf m
		hat(p_mk) is the probability of class k in leaf m 
	
	# Misclassification Error: 
	1 - hat(p_mk(m)); k(m) =  most; common; k
		* 0 = perfect purity
		* 0.5 = no purity
	
	# Gini index:
	sum_{k != k'} (hat(p_mk) * hat(p_mk') = sum(hat(p_mk) * (1-hat(p_mk)) = 1 - sum(p_mk)^2
		* 0 = perfect purity
		* 0.5 = no purity
	
	# Deviance/information gain
	- sum(hat(p_mk) * log_2(hat(p_mk))
		* 0 = perfect purity
		* 1 = no purity
	http://en.wikipedia.org/wiki/Decision_tree_learning	

	Example:
	data(iris); library(ggplot2); library(caret)
	inTrain <- createDataPartition(y=iris$Species,p=0.7, list=FALSE)
	training <- iris[inTrain,];	testing <- iris[-inTrain,]
	
	# Fit trees
	modFit <- train(Species ~ .,method="rpart",data=training)
	plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
	
	# Prettier plots
	library(rattle)
	fancyRpartPlot(modFit$finalModel)
	
	# Predict new values
	predict(modFit,newdata=testing)
	

# Bagging (Bootstrap aggregating)

	Basic idea: 
	1. Resample cases and recalculate predictions
	2. Average or majority vote
	Resample, refit then average so that the model is less variable.

	Notes:
	* Similar bias 
	* Reduced variance
	* More useful for non-linear functions

	Example:
	library(ElemStatLearn); data(ozone,package="ElemStatLearn")
	ozone <- ozone[order(ozone$ozone),]
	head(ozone)

	ll <- matrix(NA,nrow=10,ncol=155)
	for(i in 1:10){
	  ss <- sample(1:dim(ozone)[1],replace=T)
	  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
	  loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
	  ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
	}

	plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
	for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
	lines(1:155,apply(ll,2,mean),col="red",lwd=2)
	
	# Bagging in caret
	* Some models perform bagging for you, in train() function consider "method" options : bagEarth, treebag, bagFDA
	* Alternatively you can bag any model you choose using bag()
	
	# Advanced example
	predictors = data.frame(ozone=ozone$ozone)
	temperature = ozone$temperature
	treebag <- bag(predictors, temperature, B = 10,
                bagControl = bagControl(fit = ctreeBag$fit,
                                        predict = ctreeBag$pred,
                                        aggregate = ctreeBag$aggregate))
	plot(ozone$ozone,temperature,col='lightgrey',pch=19)
	points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
	points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
	http://www.inside-r.org/packages/cran/caret/docs/nbBag

	ctreeBag$fit
	ctreeBag$pred
	ctreeBag$aggregate
	
	
# Random forests

	1. Bootstrap samples
	2. At each split, bootstrap variables
	3. Grow multiple trees and vote
	+ Accuracy
	- Speed
	- Difficult to interpret
	- Overfitting

	data(iris); library(ggplot2)
	inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
	training <- iris[inTrain,]; testing <- iris[-inTrain,]

	modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
	
	# One tree
	getTree(modFit$finalModel,k=2)

	# Class "centers"
	irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
	irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
	p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
	p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)

	# Predict new values
	pred <- predict(modFit,testing); testing$predRight <- pred==testing$Species
	qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
	

# Boosting
	
	1. Take lots of (possibly) weak predictors
	2. Weight them and add them up
	3. Get a stronger predictor

	1. Start with a set of classifiers h_1,...,h_k
		* Examples: All possible trees, all possible regression models, all possible cutoffs.
	2. Create a classifier that combines classification functions:
	f(x) = sgn(sum(alpha_t * h_t(x))
	  * Goal is to minimize error (on training set)
	  * Iterative, select one h at each step
	  * Calculate weights based on errors
	  * Upweight missed classifications and select next h
  
	[Adaboost on Wikipedia](http://en.wikipedia.org/wiki/AdaBoost)
	http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf
	
	# Example
	http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf

	# Boosting on R
	gbm, mboost, ada, gamboost (available in caret)
	
	# Example
	library(ISLR); data(Wage); library(ggplot2); library(caret);
	Wage <- subset(Wage,select=-c(logwage))
	inTrain <- createDataPartition(y=Wage$wage,p=0.7, list=FALSE)
	training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
	
	modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
	qplot(predict(modFit,testing),wage,data=testing)
	

# Model based prediction

	1. Assume the data follow a probabilistic model
	2. Use Bayes' theorem to identify optimal classifiers
	+ Can take advantage of structure of the data
	+ May be computationally convenient
	+ Are reasonably accurate on real problems
	- Make additional assumptions about the data
	- When the model is incorrect you may get reduced accuracy
	
	A range of models use this approach
	* Linear discriminant analysis assumes $f_k(x)$ is multivariate Gaussian with same covariances
	* Quadratic discrimant analysis assumes $f_k(x)$ is multivariate Gaussian with different covariances
	* [Model based prediction](http://www.stat.washington.edu/mclust/) assumes more complicated versions for the covariance matrix 
	* Naive Bayes assumes independence between features for model building
	
	# Discriminant function
	delta_k(x) = x^T Sigma^{-1} mu_k - 1/2 * mu_k * Sigma^{-1} * mu_k + log(mu_k)
	mu_k is the mean, Sigma^{-1} is the inverse of the covariance function
	The largest delta_k will give us the class
	
	Example:
	data(iris); library(ggplot2);
	inTrain <- createDataPartition(y=iris$Species, p=0.7, list=FALSE)
	training <- iris[inTrain,];testing <- iris[-inTrain,]

	modlda = train(Species ~ .,data=training,method="lda")
	modnb = train(Species ~ ., data=training,method="nb")
	plda = predict(modlda,testing); pnb = predict(modnb,testing)
	
	# Plot the difference between the classification
	equalPredictions = (plda==pnb)
	qplot(Petal.Width,Sepal.Width,colour=equalPredictions,data=testing)

	
# Regularized regression
	
	1. Fit a regression model
	2. Penalize (or shrink) large coefficients
	+ Can help with the bias/variance tradeoff
	+ Can help with model selection
	- May be computationally demanding on large data sets
	- Does not perform as well as random forests and boosting
	
	Model selection approach: split samples
	Approach
		1. Divide data into training/test/validation
		2. Treat validation as test data, train all competing models on the train data and pick the best one on validation. 
		3. To appropriately assess performance on new data apply to test set
		4. You may re-split and reperform steps 1-3
	Two common problems
		Limited data
		Computational complexity
   
   Regularization for regression
	If the $\beta_j$'s are unconstrained:
		They can explode
		And hence are susceptible to very high variance
	To control variance, we might regularize/shrink the coefficients. 
	PRSS(beta) = sum(Y_j - sum(beta_1i * X_ij))^2 + P(lambda; beta)
	where PRSS is a penalized form of the sum of squares.
		* Penalty reduces complexity
		* Penalty reduces variance
		* Penalty respects structure of the problem

	In caret methods are: ridge, lasso, relaxo
	
	
# Combining predictors

	You can combine classifiers by averaging/voting
	Combining classifiers improves accuracy, reduces interpretability
	
	# Approaches for combining classifiers
	1. Bagging, boosting, random forests
		Usually combine similar classifiers
	2. Combining different classifiers
		Model stacking
		Model ensembling
	
	in R:
	mod1 <- train(wage ~.,method="glm",data=training)
	mod2 <- train(wage ~.,method="rf",data=training, trControl = trainControl(method="cv"),number=3)
	pred1 <- predict(mod1,testing); pred2 <- predict(mod2,testing)
	VS
	predDF <- data.frame(pred1,pred2,wage=testing$wage)
	combModFit <- train(wage ~.,method="gam",data=predDF)
	combPred <- predict(combModFit,predDF)

	# Error	
	sqrt(sum((pred1-testing$wage)^2))
	sqrt(sum((pred2-testing$wage)^2))
	sqrt(sum((combPred-testing$wage)^2))
	
	# Predict on validation data set
	pred1V <- predict(mod1,validation); pred2V <- predict(mod2,validation)
	predVDF <- data.frame(pred1=pred1V,pred2=pred2V)
	combPredV <- predict(combModFit,predVDF)

	sqrt(sum((pred1V-validation$wage)^2))
	sqrt(sum((pred2V-validation$wage)^2))
	sqrt(sum((combPredV-validation$wage)^2))
	
	
# Forecasting

	Data are dependent over time with specific pattern types (trends, seasonal, cycles)
	Subsampling into training/test is more complicated
	Similar issues arise in spatial data 
		Dependency between nearby observations
		Location specific effects
	Typically goal is to predict one or more observations into the future. 
	All standard predictions can be used (with caution!)

	# Economics
	library(quantmod)
	from.dat <- as.Date("01/01/08", format="%m/%d/%y")
	to.dat <- as.Date("12/31/13", format="%m/%d/%y")
	getSymbols("GOOG", src="google", from = from.dat, to = to.dat)
	
	mGoog <- to.monthly(GOOG)
	googOpen <- Op(mGoog)
	ts1 <- ts(googOpen,frequency=12)
	
	plot(ts1,xlab="Years+1", ylab="GOOG")
	plot(decompose(ts1),xlab="Years+1")
	
	ts1Train <- window(ts1,start=1,end=5)
	ts1Test <- window(ts1,start=5,end=(7-0.01))

	# Moving Average
	plot(ts1Train)
	lines(ma(ts1Train,order=3),col="red")
	
	# Exponential smoothing
	ets1 <- ets(ts1Train,model="MMM")
	fcast <- forecast(ets1)
	plot(fcast); lines(ts1Test,col="red")

	accuracy(fcast,ts1Test)

	* Rob Hyndman's [Forecasting: principles and practice](https://www.otexts.org/fpp/) is a good place to start
	* See [quantmod](http://cran.r-project.org/web/packages/quantmod/quantmod.pdf) or [quandl](http://www.quandl.com/help/packages/r) packages for finance-related problems.


# Clusters stuff

	kMeans1 <- kmeans(subset(training,select=-c(Species)),centers=3)
	training$clusters <- as.factor(kMeans1$cluster)
	qplot(Petal.Width,Petal.Length,colour=clusters,data=training)

	# Compare to real labels
	table(kMeans1$cluster,training$Species)

	# Build predictor
	modFit <- train(clusters ~.,data=subset(training,select=-c(Species)),method="rpart")
	table(predict(modFit,training),training$Species)

	# Apply on test
	testClusterPred <- predict(modFit,testing) 
	table(testClusterPred ,testing$Species)
