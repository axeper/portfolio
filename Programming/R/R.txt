# Command Line 

	# Navigate
	getwd()				
	setwd()
	ls()				# List objects in the workspace
	dir()				# List files in wd
	dir.create()		# Create folder
	unlink()			# Delete folder
	file.create() file.exists() file.info() file.copy() file.rename()
	
	rm(list = ls())		# Empty the environment
	cat("\014")  		# Clean the console

	
# Install package

	install.packages("NAME")
	install.packages(c("NAME1","NAME2"))
	Rstudio -> Tools -> Install packages...
	
	# Installing R package from Bioconductor (bioconductor.org/install/)
	source("http://bioconductor.org/biocLite.R")
	biocLite()
	biocLite(c("GenomicFeatures", "AnnotationDbi"))
	
	# Load packages
	library(NAME)
	search()
	
	# Load a .R
	source("NAME")
	ls()
	
	
# Common operations
	<-					# Assignments	
	?print				# Get help on the print function
	?`:`				# Get help on an operator
	args()				# See list of arguments
	str()				# Short help, display arguments
	table()				# Count the number of factors
	table(x,y)			# Same but two-dimensional
	summary()			# Longer help, works on objects
	object.size()		# Provides an estimate of the memory used by an R object
	quantile()			# Get information on the quantiles
	View()				# Invoke a spreadsheet viewer
	fix(x)				# Open a data editor
	
	1:20				# Integer operation
	1					# Gives a numeric object
	1L					# Gives an integer object
	Inf,	pi				
	
	attributes()		# Access the attributes of an object
	class()				# Get the class of an object
	typeof()			# Get the type of data
	is.na(), is.nan()	# Check if NA (missing), NaN
	na.omit()			# Generic way to deal with missing data
	as.logical(x)		# Explicit coertion, may result in NA
	as.numeric(x), as.character(x), as.complex
	
	plot()				# Simple x,y plot
	boxplot(m ~ c)		# Plot with mean and median with m on x-axis and c on y-axis
	hist()				# Plot histogram
	pairs()				# Plot all possible scatterplots
	pairs(~ m + d + h + w + a, DF)
	identify(x, y, name)# Click on data point to have it's name


# Data types	
	
	c()					# Generic function that combines objects
	c(0.5,0.6)
	c(TRUE, 5)			# Coercion happens, elements are casted in the same class, here numeric	
		
	vector()			# Empty vector
	vector("numeric", length = 10)	# [1] 0 0 0 0 0 0 0 0 0 0
			
	list(1, "a", TRUE, F, 1 + 4i)	# List initialization
	
	m <- matrix(1:6, nrow = 2, ncol = 3)
	dim(m)				# Create a matrix and get its dimension
	m <- 1:10
	dim(m) <- c(2,5)	# Modify an object into a matrix
	cbind(), rbind()	# Create matrix by binding columns or rows
	
	x <- factor(c("y","y","n","y","n","m"), levels = c("y","n","m"))
						# Create a factor and define the order of the levels
	table(x)			# Count how many factors
	unclass(x)			# Replace factors with numbers
	
	data.frame(a = 1:4, b = c(T,F,F,T))
	nrow(), ncol()		# Create a data frame (list of same type columns) and get its dimensions
	
	# You can name R objects: vector, list, matrix
	x <- 1:3			
	names(x) <- c("1st", "2nd", "3rd")
	list(a = 1, b = 2, c = 3)
	m <- matrix(1:4, nrow = 2, ncol = 2)
	dimnames(m) <- list(c("a","b"), c("c","d"))
	
	rep(x, times, each)	# Replicate the values of x n times, or each elements ntimes 
	paste(x)			# Concatenate the elements of x
	paste(x,y,sep="")	# Concatenate two vectors of same length, coercion may happen
	
	identical(x,y)		# Check if x is exactly the same as y
	unique(x)			# Returns the vector x but without any duplicates
	

# Subsetting
	
	x[x > "a"]			# Access index 
	u <- x > "a"		# Boolean comparison, u is a logical vector
	
	x[i]				# Extract a list
    x[[i]]				# Extract elements within the list
	x[c(i,j)]			# Extract two elements
	x[-c(i,j)]			# Extract everything but two elements
	
	x[i, ]				# Access a rows
	x[ ,j]				# Access a vecotr of a column
	x[ , j, drop=FALSE]	# Return a matrix of a column 
	x[i, j]				# Return a vector of elements i,j
	
	x$a					# Extract the list that has the element a
    x$"a"				# Extract the list that has the element "a"
	attach(x); a;		# Extract the list that has the element "a" 
	
	x$p <- y			# Add a column
	x <- cbind(x,y)		# Combine columns
	
	which(x > 5)		# Returns the indices
	x[order(x$a,x$b)]	# Order values according to column a then b (if duplicates)
	
	x %in% c("1","2")	# Same as (x == "1" | x == "2")
	
	# Partial matching
	x <- list(aardvark = 1:5)
	x$a					# Returns [1] 1 2 3 4 5
	x[["a"]]			# Returns NULL
	x[["a",exact=FALSE]]# Returns [1] 1 2 3 4 5
	
	g <- complete.cases(x, y)
	x[g]				# Remove all NA values
	x[!is.na(x) & x > 0]# Example of removing NA values and selecting positives
	
	# Logic
	which(x > 5)		# Returns the index where the element is > 5
	any(x < 0)			# Returns TRUE if at least one element of x < 0
	all(x < 0)			# Returns TRUE if all element of x < 0
	
# Vectorized operation

	x+y, x*y, x/y		# Elements-wise operations that loop
	x %*% y				# True matrix multiplication
	
	
# Reading / Writing Data
	read.table	write.table		# tabular data
	read.csv 	write.csv		# tabular data separated by commas
	readLines	writeLines		# lines of a text file
	source 		dump 			# R code files
	dget 		dput			# R code files
	load		save			# saved workspaces
	unserialize	serialize		# single R objects in binary form
	
	
	read.table important arguments:
	file, the name of a file, or a connection
	header, logical indicating if the file has a header line
	sep, a string indicating how the columns are separated
	colClasses, a character vector indicating the class of each column in the dataset
	nrows, the number of rows in the dataset
	comment.char, a character string indicating the comment character
	skip, the number of lines to skip from the beginning
	stringsAsFactors, should character variables be coded as factors?
	Note: lines beginning with # are skipped

	Reading large table is slow, use colClasses to help (e.g. colClasses = "numeric") or a somewhat dirty way
		initial <- read.table("datatable.txt", nrows = 100)
		classes <- sapply(initial, class)
		tabAll <- read.table("datatable.txt", colClasses = classes)
	
	
	# One object
	dput(y, file = "y.R")
	new.y <- dget("y.R")
	
	# Multiple objects
	dump(c("x", "y"), file = "data.R")
	rm(x, y)
	source("data.R")
	

# Interfaces
	file (name = "", open = "")	# Open a file, use "r", "w", "a", "b" for open
	
	con <- file("foo.txt", "r")
	data <- read.csv(con)
	close(con)
		OR
	data <- read.csv("foo.txt") # Same as above
	
	# Others
	gzfile, bzfile	# Open a compressed file
	url				# Open a connection to a webpage
	
	con <- gzfile("words.gz")
	x <- readLines(con, 10)
	
	con <- url("http://www.jhsph.edu", "r")
	x <- readLines(con)
	
	
# Control Structures
	
	! & | %% 		# NOT AND OR modulus 
	isTRUE(x)		# test if x is true
	
	if () {}
	else if () {}
	else {}
	
	for (X in Y) {}
	seq_along(x)	# Creates a indexed sequence for x 
	seq_len(x)		# Creates a sequence of length x
	
	while() {}
	
	repeat {}		# Behave like a while(TRUE), use break to exit
	next			# Skip an iteration
	return
	
	
# Functions

	f <- function(ARGUMENTS) {}
					# You can pass functions as arguments, and you can define a function inside another function
	(...)			# You can use ellipsis to accept a variable number of arguments
	

	# Explore the scope of a function's environment
	ls(environment(X))
	get("Y", environment(X))
	
	# Custom operator
	"%mult_add_one%" <- function(left, right){ left * right + 1 }
	4 %mult_add_one% 5	# Returns 21
	
	
# Dates and Time

	x <- Sys.Date()
	x <- as.Date("1970-01-01")
	
	x <- Sys.time()
	p <- as.POSIXct(x) # Stores as time an integer
	p <- as.POSIXlt(x, tz) # Stores time and others information
	
	# If the date is not conform use strptime
	# This allow +, -, ==, >= 
	datestring <- c("January 10, 2012 10:40", "December 9, 2011 9:10")
	x <- strptime(datestring, "%B %d, %Y %H:%M")
  
	
# Loop functions

	# apply - apply FUN on array X to be applied over MARGIN (1 rows, 2 cols, c(1,2) for both)
	apply(X, MARGIN, FUN, ...)
	
	# Note that these are way faster
	rowSums = apply(x, 1, sum)
	rowMeans = apply(x, 1, mean)
	colSums = apply(x, 2, sum)
	colMeans = apply(x, 2, mean)
	
	# lapply - apply FUN on list X (if X is not a list, it will be coerced), return a list
	lapply(X, FUN, ...)
	lapply(x, function(elt) elt[,1])	# Use an anonymous function
	
	# sapply is like lapply, but will simplify the return argument into a vector, matrix
	# vapply is similar to sapply, but a bit optimized
	# rapply is when you want to use lapply recursively into nested lists
	
	# mapply - multivariate apply, use when you want vectorized arguments
	mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)
					# ... are the arguments to apply FUN over
					# MoreArgs are the arguments for FUN
					# SIMPLIFY indicates whether to simplify the output
	mapply(FUN, 1:5, 1:5, 2)
	
	# tapply - apply FUN over INDEX subsets of a vector
	tapply(X, INDEX, FUN = NULL, ..., simplify = TRUE)
	INDEX <- c(1,1,2,2)	# For instance, this will group X into 2 groups
	
	# split - takes object x and splits it into groups according to f factors
	split(x, f, drop = FALSE, ...)
	
	# Example: split airquality for each month and apply anonymous function colMeans
	s <- split(airquality, airquality$Month)
	lapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))
	OR
	sapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))
	
	# Splitting on more than one level, here 2*5 = 10 levels
	f1 <- gl(2, 5); f2 <- gl(5, 2); interaction(f1, f2)
	split(x, list(f1, f2), drop = TRUE)
	

# Debugging

	message			# Generic notification, execution continues
	warning			# Something is wrong, execution continues
	error			# Problem occured, execution stops
	condition		# Something unexpected occured, programmers can create their own conditions
	
	invisible(x)	# Return x without printing
	
	traceback()		# Print the call stackof the last uncaught error
	debug(FUN)		# Debug a function by going line by line
	debugonce(FUN) undebug(FUN) isdebugged(FUN)
	browser()		# Interrupts the execution to browse the environment
	trace()			# Insert debugging code that can be globally de/activated
	options(error=recover)	# Allows the user to browse the environnment 
	
	
# Simulation

	rnorm(n, mean = 0, sd = 0)			
					# Generates n random normal variates
	dnorm(n, mean = 0, sd = 0)
					# Evaluate the normal probability density
	pnorm(q, mean = 0, sd = 0)
					# Evaluate the cumulative distribution function
	qnorm(p, mean = 0, sd = 0)
					# Evaluate the quantile percentile
	rpois, dpois, ppois, qpois	# Every distribution has r,d,p,q functions
	
	set.seed()		# Ensures reproductability of random results
	sample()		# Draws randomly from a set of scalars

	
# R profiler	

	system.time()	# Returns the amount of time used to evaluate the expression
					# user time = CPU time, elapsed time = real time 
									
	Rprof()			# Starts the profiler, use when the code is not too fast
	summaryRprof()	# Summarizes the results of Rprof()
	$by.total		# Time spent in the function alone and its sublevels
	$by.self		# Time spent in the function alone
	
	
# Statistics

	integrate(p,x,y)# Compute the area of pdf p from x to y
	# Create 4 quantiles on distribution
	cutpoints <- quantile(distr ,seq(0,1,length = 4))
	cut(distr, cutpoints)
	
	
	
	
	
	
	
	
	
Control structures

	for (i in start:end)
	for (j in seq_along(number))
	for (k in letter)
	
	next 		skip
	break		exit the loop
	
Functions
	fun_name <- function(var1, var2 = "default") {
		# body
	}
	
	Argument unpacking:
	args <- list(...)
	place <- args[["place"]]	

	Binary operator:
	"%m1%" <- function(left, right){
		left * right + 1
	}
	`4 %m1% 5` will return 21
	
When should I write a function or a package?
	Your closest collaborator is you six months ago, but you don't reply to emails.

	If you do X once, write code but document it very well so that it can be reproduced.
	If you do X twice, write a function to abstract input/output.
	If you do X three+, write a package with documentation.
	
	
Functional programming
	map_lgl(), map_chr(), map_dbl()
		take an input (logical, characters or double) and apply a function on it.
	
	map_if() apply the second function if the first is TRUE
		map_if(1:5, function(x){ x %% 2 == 0}, function(y){y^2}) %>% unlist()
		[1] 1 4 3 16 5
	
	map_at() apply function at certain indices (here 1, 3, 5)
		map_at(seq(100, 500, 100), c(1, 3, 5), function(x){x - 10}) %>% unlist()
		[1]  90 200 290 400 490
	
	map2_chr() apply the function to each char
		map2_chr(letters, 1:26, paste)

	The pmap() family works like map() but over a list of list.
	
	reduce()/reduce_right() starts with the first/last input then aggreagates the second with the first/last with the second to last.
	
	contains(var, pattern)
		returns TRUE if var contain pattern
	detect(start:end, function)
		returns the first result using function as a filter
	detect_index(start:end, function)
		returns the first index where fun returned TRUE
	
	keep(), discard(), every(), some() 
		returns all TRUE, all FALSE, TRUE if every/some element are TRUE
	
	compose() combines functions together
		n_unique <- compose(length, unique)
		# n_unique <- function(x){ length(unique(x))}
	
	partial() binds values to a function
		mult_three_n <- function(x, y, z){x * y * z}
		mult_by_15 <- partial(mult_three_n, x = 3, y = 5)
		mult_by_15(z = 4)
		[1] 60
	

Expressions, environment
	v <- "2 + 2"
	exp <- parse(text = v)
	eval(exp)
	deparse(exp)
	
	my_new_env <- new.env()
	assign("y", 9, envir = my_new_env)
	get("y", envir = my_new_env)
	ls(), rm(), exists(), search()
	
	<<- To assign value in the parent environment
	
	
Error Handling
	message()
	stop()
	stopifnot()			# error if not TRUE
	warning()
	
	
	
Debugging
	traceback(): prints out the function call stack after an error occurs; does nothing if there’s no error
		traceback()
	browser(): an interactive debugging environment that allows you to step through code one expression at a time
	http://stat.ethz.ch/R-manual/R-devel/library/base/html/browser.html
	debug() / debugonce(): a function that initiates the browser within a function
		debug(lm)
	trace(): this function allows you to temporarily insert pieces of code into other functions to modify their behavior
	recover(): a function for navigating the function call stack after a function has thrown an error
		options(error = recover)
		
Profiling

	# Compare two functions and plot the result
	library(microbenchmark, ggplot2)
	profile <- microbenchmark(a <- rnorm(1000), b <- mean(rnorm(1000)))
	autoplot(profile)
	
	# Identify bottlenecks
	profvis
	https://rstudio.github.io/profvis/index.html
	
	# More reading on performance
	http://adv-r.had.co.nz/Performance.html
	
	
OOP
	S3
	shape_s3 <- function(side_lengths){structure(list(side_lengths = side_lengths), class = "shape_S3")}
	is_square.shape_S3 <- function(x){
		length(x$side_lengths) == 4 && x$side_lengths[1] == x$side_lengths[2] &&
		x$side_lengths[2] == x$side_lengths[3] &&x$side_lengths[3] == x$side_lengths[4]}

	square_4 <- shape_s3(c(4, 4, 4, 4))
	class(square_4)
	[1] "shape_S3"
	is_square(square_4)

	S4 
	Fuck that
	
	
	
	
	
	
	
	R Programming Language

	# Read tabular data
	library(readr)
	teams <- read_csv("data/team_standings.csv")
	OR
	teams <- read_csv("data/team_standings.csv", col_types = "cc")
	
	logs <- read_csv("data/2016-07-20.csv.gz", col_types = "ccicccccci", n_max = 10)
	
	# Keep only one column
	logdates <- read_csv("data/2016-07-20.csv.gz", 
                     col_types = cols_only(date = col_date()),
                     n_max = 10)
					 
	read_csv	Reads comma-separated file
	read_csv2	Reads semicolon-separated file
	read_tsv	Reads tab-separated file
	read_delim	General function for reading delimited files
	read_fwf	Reads fixed width files
	read_log	Reads log files
	
	
	# Read web based data
	ext_tracks_file <- paste0("http://rammb.cira.colostate.edu/research/",
                          "tropical_cyclones/tc_extended_best_track_dataset/",
                          "data/ebtrk_atlc_1988_2015.txt")
	ext_tracks_widths <- c(7, 10, 2, 2, 3, 5, 5, 6, 4, 5, 4, 4, 5, 3, 4, 3, 3, 3,
                       4, 3, 3, 3, 4, 3, 3, 3, 2, 6, 1)
	ext_tracks_colnames <- c("storm_id", "storm_name", "month", "day",
                          "hour", "year", "latitude", "longitude",
                          "max_wind", "min_pressure", "rad_max_wind",
                          "eye_diameter", "pressure_1", "pressure_2",
                          paste("radius_34", c("ne", "se", "sw", "nw"), sep = "_"),
                          paste("radius_50", c("ne", "se", "sw", "nw"), sep = "_"),
                          paste("radius_64", c("ne", "se", "sw", "nw"), sep = "_"),
                          "storm_type", "distance_to_land", "final")

	# Read the file in from its url
	ext_tracks <- read_fwf(ext_tracks_file, 
                       fwf_widths(ext_tracks_widths, ext_tracks_colnames),
                       na = "-99")

	# Do cool stuff using dplyr
	library(dplyr)

	ext_tracks %>%
	  filter(storm_name == "KATRINA") %>%
	  select(month, day, hour, max_wind, min_pressure, rad_max_wind) %>%
	  sample_n(4)	
				   
	# Using API, example
	library(httr)
	meso_url <- "https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/"
	GET(url = meso_url, query = list(station = "DEN", data = "sped", 
		year1 = "2016", month1 = "6", day1 = "1", year2 = "2016", 
		month2 = "6", day2 = "30", tz = "America/Denver", format = "comma")) %>%
	content() %>% 
	read_csv(skip = 5, na = "M") %>% 
	slice(1:3)

	Others API
	twitteR: twitter, 
	Quandl: Quandl (financial data), 
	RGoogleAnalytics: Google Analytics
	censusr, acs: United States Census
	WDI, wbstats: World Bank
	GuardianR, rdian: The Guardian Media Group
	blsAPI: Bureau of Labor Statistics
	rtimes: New York Times
	dataRetrieval, waterData: United States Geological Survey			   
	rnoaa: National Oceanic and Atmospheric Administration	

	Web scraping: rvest (https://github.com/hadley/rvest)
	JSON, XML: jsonlite, xml2
	https://cran.r-project.org/web/views/WebTechnologies.html
	
	
	# Basic functions
	seq(start, end, by, length)
	seq_along(v)				# Return incremental sequence of length = length(v)
	rep(v, times, each)
	x[c(-2,-10)] is the same as x[-c(2,10)]
	&&, || evaluates logically only the first item
	which(x < 6), any(x < 0), all(ints > 0)
	var %in% c("aaa", "bbb")	
	
	
# Tidy data 
	package: dplyr, tidyr, lubridate, readr, stringr
	
	ext_tracks %>%
	group_by(storm_name, year) %>%
	summarize(n_obs = n(),
            worst_wind = max(max_wind),
            worst_pressure = min(min_pressure))
	
	library(ggplot2)
	ext_tracks %>%
	group_by(storm_name) %>%
	summarize(worst_wind = max(max_wind)) %>%
	ggplot(aes(x = worst_wind)) + geom_histogram() 
		
	# Advanced dplyr commands
	titanic_4 <- titanic %>% 
		select(Survived, Pclass, Age, Sex) %>%
		filter(!is.na(Age)) %>%
		mutate(agecat = cut(Age, breaks = c(0, 14.99, 50, 150), 
						  include.lowest = TRUE,
						  labels = c("Under 15", "15 to 50", "Over 50"))) %>%
		group_by(Pclass, agecat, Sex) %>%
		summarize(N = n(),
				survivors = sum(Survived == 1),
				perc_survived = 100 * survivors / N)
		
	wc_4 <- worldcup %>% 
		select(Time, Passes, Tackles, Saves) %>%
		summarize(Time = mean(Time),
				Passes = mean(Passes),
				Tackles = mean(Tackles),
				Saves = mean(Saves)) %>%
		gather(var, mean) %>%
		mutate(mean = round(mean, 1))
		
		
# Text Processing and regex
	# You can pass a vector to paste()
	two_cities <- c("best", "worst")
	paste("It was the", two_cities, "of times.")
	
	
	# regex function
	grepl(regex, string)	Return bool
	grep(r, s)				Return first match
	sub(r, r2, s)			Replace into s the first r with r2
	gsub(r, r2, s)			Same as sub but for all the r patterns
	strsplit()
	
	# metacharacters
	"."				Any single character
	"*"				Zero or more
	"+"				One or more 
	"i{2,3}"		Contains between 2 and 3 adjacent "i"
	"(iss){2,}"		Contains 2 or more adjacent "iss"
	"\\w", "\\d", "\\s"
					Contains words, digits, whitespaces
	"\\W", "\\D", "\\S" 
					Do not contain words, digits, whitespaces
	"\n", "\t"		Newline, tabs
	"[aeiou]"		Match with any lowercase vowels
	"[^aeiou]"		Match with anything but lowercase vowels
	"[a-mA-M]"		Match with any letter
	"^a", "a$"		Begins with, ends with
	"a|b"			Match with a or b
	
	exemple: start with a capitaized vowel, ends with a vowel
	start_end_vowel <- "^[AEIOU]{1}.+[aeiou]{1}$"
	
	# stingr
	library(stringr)
	
	str_extract(s, regex)	Extract the regex from s
	str_order(s)			Returns indices so that s is in alphabetical order
	str_pad("Thai", width = 8, side = "both", pad = "-")
	[1] "--Thai--"			Pad a string left/right/both
	str_to_title()			Capitalize a string
	str_trim()				Remove whitespace from both side of a string
	str_wrap(s, width = 80)	Insert a newline as soon as the width is exceeded
	word(s, start, end)		Allows to index a sentence using start, end
	
	Human readable regex: rex
	https://github.com/kevinushey/rex
	Test regex online
	http://regexr.com/
	Tidy Text Mining with R
	http://tidytextmining.com/index.html
	
	
# Memory usage
	library(pryr)
	mem_used()
	object_size(var)
	
	# Print the 5 largest objects
	library(magrittr)
	sapply(ls(), function(x) object_size(get(x))) %>% sort %>% tail(5)
	
	# remove a variable and see how much it left
	mem_change(rm(var))
	
	# R has an automatic garbage collection, you can print some stuff with
	gc()
	
	
# Working with large datasets	
	library(data.table)
	brazil_zika <- fread("data/COES_Microcephaly-2016-06-25.csv")
	
	fread("data/COES_Microcephaly-2016-06-25.csv",
      select = c("location", "value", "unit")) %>%
	  dplyr::slice(1:3)
	
	For optimal code, use Rcpp so you can bridge R to C++.
	
	Find parallelized package with
	High-Performance and Parallel Computing with R task view.
	https://cran.r-project.org/web/views/HighPerformanceComputing.html
	
	If the data is too large, use a RDBMS
	R generic package 	DBI
	R specific package	RMySQL, ROracle, RSQLServer, RSQLite, RPostgres
	
	